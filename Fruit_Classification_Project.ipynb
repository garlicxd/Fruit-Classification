{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garlicxd/Fruit-Classification/blob/main/Fruit_Classification_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE5TzdybiHaZ"
      },
      "source": [
        "## Todo\n",
        "- [ ] patience for training - stopping after no improvement for several epochs\n",
        "- [ ] create graphs - store information while training\n",
        "- [ ] compare different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXL15tF4sDbz"
      },
      "outputs": [],
      "source": [
        "# @title Download Dataset\n",
        "\n",
        "# Define the path for the target dataset folder\n",
        "dataset_folder_path = \"./split_ttv_dataset_type_of_plants\"\n",
        "\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "import os\n",
        "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "kaggle_json_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "if not os.path.exists(kaggle_json_path):\n",
        "    print(\"kaggle.json not found. Please upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "    if \"kaggle.json\" in uploaded:\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !cp kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "        print(\"Kaggle API key configured successfully!\")\n",
        "    else:\n",
        "        print(\"Upload failed or kaggle.json not found in upload.\")\n",
        "else:\n",
        "    print(\"Kaggle API key already configured.\")\n",
        "\n",
        "if not os.path.exists(dataset_folder_path):\n",
        "    print(f\"Dataset folder '{dataset_folder_path}' not found. Downloading and unzipping...\")\n",
        "    !kaggle datasets download -d yudhaislamisulistya/plants-type-datasets\n",
        "    !unzip -q plants-type-datasets.zip\n",
        "    print(\"Dataset downloaded and unzipped successfully.\")\n",
        "else:\n",
        "    print(f\"Dataset folder '{dataset_folder_path}' already exists. Skipping download and unzip.\")\n",
        "\n",
        "print(\"\\nClasses in training directory:\")\n",
        "!ls ./split_ttv_dataset_type_of_plants/Train_Set_Folder/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiNG-xH1QmJC"
      },
      "outputs": [],
      "source": [
        "# @title Parameters and Includes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "BASE_DIR = './split_ttv_dataset_type_of_plants'\n",
        "BASE_TRAIN_DIR = os.path.join(BASE_DIR, 'Train_Set_Folder')\n",
        "BASE_VAL_DIR = os.path.join(BASE_DIR, 'Validation_Set_Folder')\n",
        "BASE_TEST_DIR = os.path.join(BASE_DIR, 'Test_Set_Folder')\n",
        "SHUFFLE_TRAINING = True\n",
        "\n",
        "CLASSES_TO_USE = [\"aloevera\", \"banana\", \"bilimbi\", \"cantaloupe\", \"cassava\", \"coconut\", \"corn\", \"cucumber\", \"curcuma\", \"eggplant\", \"galangal\", \"ginger\", \"guava\", \"kale\", \"longbeans\", \"mango\", \"melon\", \"orange\", \"paddy\", \"papaya\", \"peper chili\", \"pineapple\", \"pomelo\", \"shallot\", \"soybeans\", \"spinach\", \"sweet potatoes\", \"tobacco\", \"waterapple\", \"watermelon\"]\n",
        "\n",
        "MAX_IMAGES_PER_CLASS_TRAIN = 1000\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 5\n",
        "MODEL_SAVE_PATH = './barebones_resnet50.pth'\n",
        "CLASS_MAP_PATH = './class_mapping.json'\n",
        "\n",
        "# TOGGLE THIS BETWEEN RUNS\n",
        "IMG_AUGMENT = True\n",
        "\n",
        "# Experiment tracking based on IMG_AUGMENT\n",
        "EXPERIMENT_NAME = \"with_augmented_images\" if IMG_AUGMENT else \"baseline_no_augmented_images\"\n",
        "MODEL_SAVE_PATH = f'./resnet50_{EXPERIMENT_NAME}.pth'\n",
        "CLASS_MAP_PATH = './class_mapping.json'\n",
        "HISTORY_SAVE_PATH = f'./history_{EXPERIMENT_NAME}.json'\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Classes to train: {', '.join(CLASSES_TO_USE)}\")\n",
        "print(f\"Max training images per class: {MAX_IMAGES_PER_CLASS_TRAIN}\")\n",
        "print(f\"Include augmented images: {IMG_AUGMENT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovhMGNJaWf31"
      },
      "outputs": [],
      "source": [
        "# @title Filtered Set Creation\n",
        "\n",
        "base_filtered_dir = './filtered_data'\n",
        "filtered_train_dir = os.path.join(base_filtered_dir, 'train')\n",
        "filtered_val_dir = os.path.join(base_filtered_dir, 'val')\n",
        "filtered_test_dir = os.path.join(base_filtered_dir, 'test')\n",
        "\n",
        "if os.path.exists(base_filtered_dir):\n",
        "    print(f\"Removing existing filtered data directory: {base_filtered_dir}\")\n",
        "    shutil.rmtree(base_filtered_dir)\n",
        "\n",
        "class_to_idx = {}\n",
        "idx_to_class = []\n",
        "\n",
        "print(\"Creating filtered training set...\")\n",
        "for i, class_name in enumerate(CLASSES_TO_USE):\n",
        "    class_to_idx[class_name] = i\n",
        "    idx_to_class.append(class_name)\n",
        "\n",
        "    source_dir = os.path.join(BASE_TRAIN_DIR, class_name)\n",
        "    dest_dir = os.path.join(filtered_train_dir, class_name)\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    all_images = glob.glob(os.path.join(source_dir, '*.*'))\n",
        "    if IMG_AUGMENT:\n",
        "        images_to_copy = [img for img in all_images if os.path.basename(img).startswith('aug_')][:MAX_IMAGES_PER_CLASS_TRAIN]\n",
        "    else:\n",
        "        images_to_copy = [img for img in all_images if not os.path.basename(img).startswith('aug_')][:MAX_IMAGES_PER_CLASS_TRAIN]\n",
        "\n",
        "\n",
        "    for img_path in images_to_copy:\n",
        "        shutil.copy(img_path, dest_dir)\n",
        "print(f\"Filtered training set created at: {filtered_train_dir}\")\n",
        "\n",
        "print(\"Creating filtered validation set...\")\n",
        "for class_name in CLASSES_TO_USE:\n",
        "    source_dir = os.path.join(BASE_VAL_DIR, class_name)\n",
        "    dest_dir = os.path.join(filtered_val_dir, class_name)\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    all_images = glob.glob(os.path.join(source_dir, '*.*'))\n",
        "    if IMG_AUGMENT:\n",
        "        images_to_copy = [img for img in all_images if os.path.basename(img).startswith('aug_')]\n",
        "    else:\n",
        "        images_to_copy = [img for img in all_images if not os.path.basename(img).startswith('aug_')]\n",
        "\n",
        "    for img_path in images_to_copy:\n",
        "        shutil.copy(img_path, dest_dir)\n",
        "print(f\"Filtered validation set created at: {filtered_val_dir}\")\n",
        "\n",
        "print(\"Creating filtered test set...\")\n",
        "for class_name in CLASSES_TO_USE:\n",
        "    source_dir = os.path.join(BASE_TEST_DIR, class_name)\n",
        "    dest_dir = os.path.join(filtered_test_dir, class_name)\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    all_images = glob.glob(os.path.join(source_dir, '*.*'))\n",
        "    if IMG_AUGMENT:\n",
        "        images_to_copy = [img for img in all_images if os.path.basename(img).startswith('aug_')]\n",
        "    else:\n",
        "        images_to_copy = [img for img in all_images if not os.path.basename(img).startswith('aug_')]\n",
        "\n",
        "    for img_path in images_to_copy:\n",
        "        shutil.copy(img_path, dest_dir)\n",
        "print(f\"Filtered test set created at: {filtered_test_dir}\")\n",
        "\n",
        "with open(CLASS_MAP_PATH, 'w') as f:\n",
        "    json.dump(class_to_idx, f)\n",
        "print(f\"Class mapping saved to {CLASS_MAP_PATH}\")\n",
        "\n",
        "NUM_CLASSES = len(CLASSES_TO_USE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bklcIHJYd1k"
      },
      "outputs": [],
      "source": [
        "# @title Define Transforms and DataLoaders\n",
        "\n",
        "imgnet_mean = [0.485, 0.456, 0.406]\n",
        "imgnet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imgnet_mean, std=imgnet_std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imgnet_mean, std=imgnet_std)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imgnet_mean, std=imgnet_std)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create datasets\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(filtered_train_dir, data_transforms['train']),\n",
        "    'val': datasets.ImageFolder(filtered_val_dir, data_transforms['val']),\n",
        "    'test': datasets.ImageFolder(filtered_test_dir, data_transforms['test'])\n",
        "}\n",
        "\n",
        "# Create dataloaders\n",
        "dataloaders = {\n",
        "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=SHUFFLE_TRAINING),\n",
        "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False),\n",
        "    'test': DataLoader(image_datasets['test'], batch_size=BATCH_SIZE, shuffle=False)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "print(f\"Total training images: {dataset_sizes['train']}\")\n",
        "print(f\"Total validation images: {dataset_sizes['val']}\")\n",
        "print(f\"Total test images: {dataset_sizes['test']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRrVFmF0aFks"
      },
      "outputs": [],
      "source": [
        "# @title Define the Model (ResNet50) and Freeze Layers\n",
        "\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False #freezes\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(\"Model definition complete. New final layer:\")\n",
        "print(model.fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GQ-REHuaP78"
      },
      "outputs": [],
      "source": [
        "# @title Define Loss and Optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Should use Adam for best results but check SGD parameters first\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=LEARNING_RATE)\n",
        "print(f\"Using optimizer: SGD with LR={LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjWIyFcfab4Z"
      },
      "outputs": [],
      "source": [
        "# @title Training Loop (with Validation)\n",
        "\n",
        "print(\"Starting training...\") # Indicate the start of the training process\n",
        "start_time = time.time() # Record the start time of training\n",
        "\n",
        "# To store best model weights\n",
        "best_model_wts = model.state_dict() # Initialize with the current model's state dictionary\n",
        "best_val_acc = 0.0 # Initialize best validation accuracy to 0\n",
        "\n",
        "# Dictionary to store training history for plotting\n",
        "history = {\n",
        "    \"loss\": [], # List to store training loss per epoch\n",
        "    \"accuracy\": [], # List to store training accuracy per epoch\n",
        "    \"val_loss\": [], # List to store validation loss per epoch\n",
        "    \"val_accuracy\": [] # List to store validation accuracy per epoch\n",
        "}\n",
        "\n",
        "# Loop through the specified number of epochs\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\") # Print current epoch number\n",
        "\n",
        "    # Training Phase\n",
        "    model.train() # Set the model to training mode (enables gradients and dropout)\n",
        "\n",
        "    running_loss = 0.0 # Initialize running loss for the epoch\n",
        "    running_corrects = 0 # Initialize running correct predictions for the epoch\n",
        "\n",
        "    # Create a progress bar for the training dataloader\n",
        "    progress_bar = tqdm(dataloaders['train'], desc=\"[Train]\")\n",
        "\n",
        "    # Iterate over data in the training dataloader\n",
        "    for inputs, labels in progress_bar:\n",
        "        inputs = inputs.to(DEVICE) # Move inputs to the specified device (CPU/GPU)\n",
        "        labels = labels.to(DEVICE) # Move labels to the specified device\n",
        "\n",
        "        optimizer.zero_grad() # Zero the gradients of the optimizer\n",
        "\n",
        "        outputs = model(inputs) # Forward pass: compute model outputs\n",
        "        loss = criterion(outputs, labels) # Calculate the loss\n",
        "        _, preds = torch.max(outputs, 1) # Get the predicted class (index of max logit)\n",
        "\n",
        "        loss.backward() # Backpropagation: compute gradients\n",
        "        optimizer.step() # Update model parameters\n",
        "\n",
        "        batch_loss = loss.item() # Get the scalar value of the loss for the current batch\n",
        "        running_loss += batch_loss * inputs.size(0) # Accumulate batch loss weighted by batch size\n",
        "        running_corrects += torch.sum(preds == labels.data) # Accumulate correct predictions\n",
        "\n",
        "        # Update progress bar with current batch loss\n",
        "        progress_bar.set_postfix(batch_loss=f\"{batch_loss:.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / dataset_sizes['train'] # Calculate average training loss for the epoch\n",
        "    epoch_acc = running_corrects.double() / dataset_sizes['train'] # Calculate training accuracy for the epoch\n",
        "\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval() # Set the model to evaluation mode (disables gradients and dropout)\n",
        "\n",
        "    val_running_loss = 0.0 # Initialize running loss for the validation set\n",
        "    val_running_corrects = 0 # Initialize running correct predictions for the validation set\n",
        "\n",
        "    # Disable gradient calculations for current epoch validation\n",
        "    # Increases running speed\n",
        "    with torch.no_grad():\n",
        "        # Create a progress bar for the validation dataloader\n",
        "        progress_bar_val = tqdm(dataloaders['val'], desc=\"[Validate]\")\n",
        "        # Iterate over data in the validation dataloader\n",
        "        for inputs, labels in progress_bar_val:\n",
        "            inputs = inputs.to(DEVICE) # Move inputs to the specified device\n",
        "            labels = labels.to(DEVICE) # Move labels to the specified device\n",
        "\n",
        "            outputs = model(inputs) # Forward pass: compute model outputs\n",
        "            loss = criterion(outputs, labels) # Calculate the loss\n",
        "            _, preds = torch.max(outputs, 1) # Get the predicted class\n",
        "\n",
        "            val_running_loss += loss.item() * inputs.size(0) # Accumulate batch validation loss\n",
        "            val_running_corrects += torch.sum(preds == labels.data) # Accumulate correct validation predictions\n",
        "\n",
        "    val_loss = val_running_loss / dataset_sizes['val'] # Calculate average validation loss for the epoch\n",
        "    val_acc = val_running_corrects.double() / dataset_sizes['val'] # Calculate validation accuracy for the epoch\n",
        "\n",
        "    # Status update per epoch\n",
        "    print(f\"Epoch Summary:\") # Print summary header\n",
        "    print(f\"  Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}\") # Print training loss and accuracy\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\") # Print validation loss and accuracy\n",
        "\n",
        "    # saving evaluation values for plotting graphs later\n",
        "    history[\"loss\"].append(epoch_loss) # Append training loss to history\n",
        "    history[\"accuracy\"].append(epoch_acc.item() if hasattr(epoch_acc, \"item\") else float(epoch_acc)) # Append training accuracy to history\n",
        "    history[\"val_loss\"].append(val_loss) # Append validation loss to history\n",
        "    history[\"val_accuracy\"].append(val_acc.item() if hasattr(val_acc, \"item\") else float(val_acc)) # Append validation accuracy to history\n",
        "\n",
        "\n",
        "    # Save the model if it has the best validation accuracy so far\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc # Update best validation accuracy\n",
        "        best_model_wts = model.state_dict() # Save the model's state dictionary\n",
        "        print(f\"  -> New best model found! (Val Acc: {best_val_acc:.4f})\") # Indicate new best model found\n",
        "\n",
        "time_elapsed = time.time() - start_time # Calculate total training time\n",
        "print(f\"\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\") # Print total training time\n",
        "print(f\"Best Val Acc: {best_val_acc:.4f}\") # Print the best validation accuracy achieved\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts) # Load the state dictionary of the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvufyeTq4PMX"
      },
      "outputs": [],
      "source": [
        "# @title Save Training History for Comparison\n",
        "\n",
        "import json\n",
        "\n",
        "# Save history to file for later comparison\n",
        "history_to_save = {\n",
        "    'loss': [float(x) for x in history['loss']],\n",
        "    'accuracy': [float(x) for x in history['accuracy']],\n",
        "    'val_loss': [float(x) for x in history['val_loss']],\n",
        "    'val_accuracy': [float(x) for x in history['val_accuracy']],\n",
        "    'experiment_name': EXPERIMENT_NAME,\n",
        "    'img_augment': IMG_AUGMENT,\n",
        "    'epochs': EPOCHS,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'best_val_acc': float(best_val_acc)\n",
        "}\n",
        "\n",
        "with open(HISTORY_SAVE_PATH, 'w') as f:\n",
        "    json.dump(history_to_save, f, indent=2)\n",
        "\n",
        "print(f\"\\nTraining history saved to {HISTORY_SAVE_PATH}\")\n",
        "print(f\"  Experiment: {EXPERIMENT_NAME}\")\n",
        "print(f\"  Best Val Acc: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrMaI9e6bbRU"
      },
      "outputs": [],
      "source": [
        "# @title Save Best Model\n",
        "\n",
        "print(f\"Saving best model state dictionary to {MODEL_SAVE_PATH}...\")\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(\"Best model saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR1Q9M5YbjmQ"
      },
      "outputs": [],
      "source": [
        "# @title Load Model\n",
        "\n",
        "print(\"--- Loading Model for Inference ---\")\n",
        "\n",
        "print(f\"Loading class mapping from {CLASS_MAP_PATH}...\")\n",
        "with open(CLASS_MAP_PATH, 'r') as f:\n",
        "    loaded_class_to_idx = json.load(f)\n",
        "LOADED_NUM_CLASSES = len(loaded_class_to_idx)\n",
        "print(f\"Loaded {LOADED_NUM_CLASSES} classes.\")\n",
        "\n",
        "model_to_load = models.resnet50(weights=None)\n",
        "num_ftrs = model_to_load.fc.in_features\n",
        "model_to_load.fc = nn.Linear(num_ftrs, LOADED_NUM_CLASSES)\n",
        "\n",
        "print(f\"Loading model weights from {MODEL_SAVE_PATH}...\")\n",
        "model_to_load.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "model_to_load = model_to_load.to(DEVICE)\n",
        "model_to_load.eval()\n",
        "\n",
        "print(\"Model loaded successfully and set to evaluation mode.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS4jzkBAdVAV"
      },
      "outputs": [],
      "source": [
        "# @title 3. Final Testing on Test Set\n",
        "\n",
        "print(\"--- Running Final Evaluation on Test Set ---\")\n",
        "\n",
        "test_running_loss = 0.0\n",
        "test_running_corrects = 0\n",
        "\n",
        "# Get a reverse mapping from index to class name\n",
        "loaded_idx_to_class = {v: k for k, v in loaded_class_to_idx.items()}\n",
        "\n",
        "# Set model to evaluation mode and disable gradients\n",
        "model_to_load.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    progress_bar_test = tqdm(dataloaders['test'], desc=\"[Test]\")\n",
        "\n",
        "    for inputs, labels in progress_bar_test:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model_to_load(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        test_running_loss += loss.item() * inputs.size(0)\n",
        "        test_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "# Calculate final test statistics\n",
        "final_test_loss = test_running_loss / dataset_sizes['test']\n",
        "final_test_acc = test_running_corrects.double() / dataset_sizes['test']\n",
        "\n",
        "print(\"\\n--- Final Test Results ---\")\n",
        "print(f\"  Test Loss: {final_test_loss:.4f}\")\n",
        "print(f\"  Test Acc:  {final_test_acc:.4f} ({test_running_corrects.item()}/{dataset_sizes['test']})\")\n",
        "\n",
        "# Save test results to history file\n",
        "with open(HISTORY_SAVE_PATH, 'r') as f:\n",
        "    history_data = json.load(f)\n",
        "\n",
        "history_data['test_loss'] = float(final_test_loss)\n",
        "history_data['test_accuracy'] = float(final_test_acc)\n",
        "\n",
        "with open(HISTORY_SAVE_PATH, 'w') as f:\n",
        "    json.dump(history_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nTest results saved to {HISTORY_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0sFSAvIcBvH"
      },
      "source": [
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT0VB9UWaaQ7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_history(history):\n",
        "    plt.figure(figsize=(20,6))\n",
        "\n",
        "    epochs = range(1, len(history['accuracy']) + 1)\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    plt.subplot(121)\n",
        "    plt.plot(epochs, history['accuracy'])\n",
        "    plt.plot(epochs, history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.subplot(122)\n",
        "    plt.plot(epochs, history['loss'])\n",
        "    plt.plot(epochs, history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "show_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub3OyCY8WW2h"
      },
      "outputs": [],
      "source": [
        "# Redefine imshow to handle normalization\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    #undo normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1K5-4vYuUJb"
      },
      "outputs": [],
      "source": [
        "# Generic function to display predictions for a few images\n",
        "def visualize_model(model, num_images=10):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['train']):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            print(\"preds: \", len(preds))\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(2, num_images//2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('pred/true: {}/{}'.format(CLASSES_TO_USE[preds[j]],\n",
        "                                                       CLASSES_TO_USE[labels[j]]))\n",
        "                print(CLASSES_TO_USE[preds[j]] == CLASSES_TO_USE[labels[j]])\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39KynkuIrO1I"
      },
      "outputs": [],
      "source": [
        "visualize_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5CbkhGbcEfu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, normalize=False, title=None, cmap=plt.cm.Blues):\n",
        "    if title is None:\n",
        "        title = 'Normalized confusion matrix' if normalize else 'Confusion matrix, without normalization'\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))\n",
        "\n",
        "    if normalize:\n",
        "        with np.errstate(all='ignore'):\n",
        "            cm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "            cm = np.nan_to_num(cm)  # handle rows with zero support\n",
        "\n",
        "    width = min(24, 0.6 * len(class_names))\n",
        "    fig, ax = plt.subplots(figsize=(width, 10))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap, aspect='auto')\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    ax.set(\n",
        "        xticks=np.arange(len(class_names)),\n",
        "        yticks=np.arange(len(class_names)),\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        title=title,\n",
        "        ylabel='True label',\n",
        "        xlabel='Predicted label'\n",
        "    )\n",
        "    ax.tick_params(axis='x', labelrotation=45, pad=12)\n",
        "    ax.tick_params(axis='y', pad=6)\n",
        "\n",
        "    # give labels extra margins so they don’t clip\n",
        "    fig.subplots_adjust(bottom=0.32, left=0.28)\n",
        "\n",
        "    # annotations (smaller font helps clutter)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.0 if cm.size else 0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    fontsize=8,\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "# --- Collect predictions over the FULL dataset, not just the last batch ---\n",
        "model.eval()\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, y_true in dataloaders['train']:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true_all.append(y_true.cpu().numpy())\n",
        "        y_pred_all.append(preds.cpu().numpy())\n",
        "\n",
        "y_true_all = np.concatenate(y_true_all)\n",
        "y_pred_all = np.concatenate(y_pred_all)\n",
        "\n",
        "print(len(y_true_all))\n",
        "print(len(y_pred_all))\n",
        "# class_names must be a list of names ordered by the class index used by the model\n",
        "# If you have a torchvision ImageFolder:\n",
        "#   idx_to_class = {v:k for k, v in dataset.class_to_idx.items()}\n",
        "#   class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "class_names = CLASSES_TO_USE  # ensure this is a list of strings in index order\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot\n",
        "plot_confusion_matrix(y_true_all, y_pred_all, class_names, normalize=False,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_true_all, y_pred_all, class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fUs1Y0M4PMf"
      },
      "outputs": [],
      "source": [
        "# @title Data Distribution Analysis\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Count samples per class for each split\n",
        "def count_class_samples(dataset_dir):\n",
        "    \"\"\"Count number of samples per class in a directory.\"\"\"\n",
        "    class_counts = {}\n",
        "    for class_name in CLASSES_TO_USE:\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        if os.path.exists(class_dir):\n",
        "            num_images = len(glob.glob(os.path.join(class_dir, '*.*')))\n",
        "            class_counts[class_name] = num_images\n",
        "        else:\n",
        "            class_counts[class_name] = 0\n",
        "    return class_counts\n",
        "\n",
        "# Get counts for all splits\n",
        "train_counts = count_class_samples(filtered_train_dir)\n",
        "val_counts = count_class_samples(filtered_val_dir)\n",
        "test_counts = count_class_samples(filtered_test_dir)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "# Training set distribution\n",
        "axes[0].bar(CLASSES_TO_USE, train_counts.values(), color='steelblue')\n",
        "axes[0].set_title('Training Set Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Number of Samples')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Validation set distribution\n",
        "axes[1].bar(CLASSES_TO_USE, val_counts.values(), color='orange')\n",
        "axes[1].set_title('Validation Set Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Number of Samples')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Test set distribution\n",
        "axes[2].bar(CLASSES_TO_USE, test_counts.values(), color='green')\n",
        "axes[2].set_title('Test Set Distribution', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Number of Samples')\n",
        "axes[2].set_xlabel('Class Name')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(\"\\n=== Dataset Distribution Statistics ===\\n\")\n",
        "\n",
        "print(\"TRAINING SET:\")\n",
        "total_train = sum(train_counts.values())\n",
        "for class_name, count in sorted(train_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = 100 * count / total_train if total_train > 0 else 0\n",
        "    print(f\"  {class_name:20s}: {count:4d} samples ({percentage:5.2f}%)\")\n",
        "print(f\"  {'TOTAL':20s}: {total_train:4d} samples\")\n",
        "\n",
        "print(\"\\nVALIDATION SET:\")\n",
        "total_val = sum(val_counts.values())\n",
        "for class_name, count in sorted(val_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = 100 * count / total_val if total_val > 0 else 0\n",
        "    print(f\"  {class_name:20s}: {count:4d} samples ({percentage:5.2f}%)\")\n",
        "print(f\"  {'TOTAL':20s}: {total_val:4d} samples\")\n",
        "\n",
        "print(\"\\nTEST SET:\")\n",
        "total_test = sum(test_counts.values())\n",
        "for class_name, count in sorted(test_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = 100 * count / total_test if total_test > 0 else 0\n",
        "    print(f\"  {class_name:20s}: {count:4d} samples ({percentage:5.2f}%)\")\n",
        "print(f\"  {'TOTAL':20s}: {total_test:4d} samples\")\n",
        "\n",
        "# Check for imbalance\n",
        "train_counts_list = list(train_counts.values())\n",
        "if train_counts_list:\n",
        "    max_count = max(train_counts_list)\n",
        "    min_count = min(train_counts_list)\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "\n",
        "    print(f\"\\n=== Imbalance Analysis ===\")\n",
        "    print(f\"Training set imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "    if imbalance_ratio > 3:\n",
        "        print(\"WARNING: Significant class imbalance detected!\")\n",
        "        print(\"Consider using class weights or data augmentation.\")\n",
        "    elif imbalance_ratio > 1.5:\n",
        "        print(\"Moderate class imbalance detected.\")\n",
        "        print(\"Monitor per-class performance in confusion matrix.\")\n",
        "    else:\n",
        "        print(\"Dataset is reasonably balanced.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OhD5Uzj4PMf"
      },
      "outputs": [],
      "source": [
        "# @title Compare Baseline vs Augmented Results\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Try to load both history files\n",
        "baseline_path = './history_baseline_no_augmented_images.json'\n",
        "augmented_path = './history_with_augmented_images.json'\n",
        "\n",
        "try:\n",
        "    with open(baseline_path, 'r') as f:\n",
        "        history_baseline = json.load(f)\n",
        "    baseline_exists = True\n",
        "    print(f\"✓ Loaded baseline results\")\n",
        "except FileNotFoundError:\n",
        "    baseline_exists = False\n",
        "    print(f\"✗ Baseline results not found at {baseline_path}\")\n",
        "\n",
        "try:\n",
        "    with open(augmented_path, 'r') as f:\n",
        "        history_augmented = json.load(f)\n",
        "    augmented_exists = True\n",
        "    print(f\"✓ Loaded augmented results\")\n",
        "except FileNotFoundError:\n",
        "    augmented_exists = False\n",
        "    print(f\"✗ Augmented results not found at {augmented_path}\")\n",
        "\n",
        "if not (baseline_exists and augmented_exists):\n",
        "    print(\"\\n⚠️  You need to run training twice:\")\n",
        "    print(\"   1. First run with IMG_AUGMENT = False\")\n",
        "    print(\"   2. Second run with IMG_AUGMENT = True\")\n",
        "    print(\"   Then run this cell again to compare.\")\n",
        "else:\n",
        "    # ==================== PLOTTING ====================\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "    epochs_base = range(1, len(history_baseline['accuracy']) + 1)\n",
        "    epochs_aug = range(1, len(history_augmented['accuracy']) + 1)\n",
        "\n",
        "    # 1. Training Accuracy Comparison\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    ax1.plot(epochs_base, history_baseline['accuracy'], 'b-o', label='Baseline', linewidth=2)\n",
        "    ax1.plot(epochs_aug, history_augmented['accuracy'], 'r-s', label='With Augmented Images', linewidth=2)\n",
        "    ax1.set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Validation Accuracy Comparison\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    ax2.plot(epochs_base, history_baseline['val_accuracy'], 'b-o', label='Baseline', linewidth=2)\n",
        "    ax2.plot(epochs_aug, history_augmented['val_accuracy'], 'r-s', label='With Augmented Images', linewidth=2)\n",
        "    ax2.set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Overfitting Gap (Train - Val Accuracy)\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    gap_baseline = [t - v for t, v in zip(history_baseline['accuracy'], history_baseline['val_accuracy'])]\n",
        "    gap_augmented = [t - v for t, v in zip(history_augmented['accuracy'], history_augmented['val_accuracy'])]\n",
        "    ax3.plot(epochs_base, gap_baseline, 'b-o', label='Baseline Gap', linewidth=2)\n",
        "    ax3.plot(epochs_aug, gap_augmented, 'r-s', label='Augmented Gap', linewidth=2)\n",
        "    ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax3.set_title('Overfitting Gap (Train - Val Acc)', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('Gap')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Training Loss Comparison\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    ax4.plot(epochs_base, history_baseline['loss'], 'b-o', label='Baseline', linewidth=2)\n",
        "    ax4.plot(epochs_aug, history_augmented['loss'], 'r-s', label='With Augmented Images', linewidth=2)\n",
        "    ax4.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('Loss')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Validation Loss Comparison\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    ax5.plot(epochs_base, history_baseline['val_loss'], 'b-o', label='Baseline', linewidth=2)\n",
        "    ax5.plot(epochs_aug, history_augmented['val_loss'], 'r-s', label='With Augmented Images', linewidth=2)\n",
        "    ax5.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
        "    ax5.set_xlabel('Epoch')\n",
        "    ax5.set_ylabel('Loss')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Summary Bar Chart\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    metrics = ['Train Acc', 'Val Acc', 'Test Acc', 'Train-Val Gap']\n",
        "    baseline_vals = [\n",
        "        history_baseline['accuracy'][-1],\n",
        "        history_baseline['val_accuracy'][-1],\n",
        "        history_baseline.get('test_accuracy', 0),\n",
        "        gap_baseline[-1]\n",
        "    ]\n",
        "    augmented_vals = [\n",
        "        history_augmented['accuracy'][-1],\n",
        "        history_augmented['val_accuracy'][-1],\n",
        "        history_augmented.get('test_accuracy', 0),\n",
        "        gap_augmented[-1]\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "    ax6.bar(x - width/2, baseline_vals, width, label='Baseline', color='steelblue')\n",
        "    ax6.bar(x + width/2, augmented_vals, width, label='With Augmented Images', color='coral')\n",
        "    ax6.set_ylabel('Value')\n",
        "    ax6.set_title('Final Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "    ax6.set_xticks(x)\n",
        "    ax6.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ==================== NUMERICAL COMPARISON ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"                    DETAILED COMPARISON REPORT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nBASELINE (No Augmented Images):\")\n",
        "    print(f\"  Final Train Acc:     {history_baseline['accuracy'][-1]:.4f}\")\n",
        "    print(f\"  Final Val Acc:       {history_baseline['val_accuracy'][-1]:.4f}\")\n",
        "    print(f\"  Best Val Acc:        {history_baseline['best_val_acc']:.4f}\")\n",
        "    if 'test_accuracy' in history_baseline:\n",
        "        print(f\"  Test Acc:            {history_baseline['test_accuracy']:.4f}\")\n",
        "    print(f\"  Train-Val Gap:       {gap_baseline[-1]:.4f}\")\n",
        "    print(f\"  Final Train Loss:    {history_baseline['loss'][-1]:.4f}\")\n",
        "    print(f\"  Final Val Loss:      {history_baseline['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    print(\"\\nWITH AUGMENTED IMAGES:\")\n",
        "    print(f\"  Final Train Acc:     {history_augmented['accuracy'][-1]:.4f}\")\n",
        "    print(f\"  Final Val Acc:       {history_augmented['val_accuracy'][-1]:.4f}\")\n",
        "    print(f\"  Best Val Acc:        {history_augmented['best_val_acc']:.4f}\")\n",
        "    if 'test_accuracy' in history_augmented:\n",
        "        print(f\"  Test Acc:            {history_augmented['test_accuracy']:.4f}\")\n",
        "    print(f\"  Train-Val Gap:       {gap_augmented[-1]:.4f}\")\n",
        "    print(f\"  Final Train Loss:    {history_augmented['loss'][-1]:.4f}\")\n",
        "    print(f\"  Final Val Loss:      {history_augmented['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    print(\"\\nIMPROVEMENTS (Augmented - Baseline):\")\n",
        "    val_improvement = history_augmented['best_val_acc'] - history_baseline['best_val_acc']\n",
        "    gap_reduction = gap_baseline[-1] - gap_augmented[-1]\n",
        "\n",
        "    print(f\"  Best Val Acc Change:     {val_improvement:+.4f} ({val_improvement*100:+.2f}%)\")\n",
        "    print(f\"  Overfitting Reduction:   {gap_reduction:+.4f} ({gap_reduction*100:+.2f}%)\")\n",
        "\n",
        "    if 'test_accuracy' in history_baseline and 'test_accuracy' in history_augmented:\n",
        "        test_improvement = history_augmented['test_accuracy'] - history_baseline['test_accuracy']\n",
        "        print(f\"  Test Acc Change:         {test_improvement:+.4f} ({test_improvement*100:+.2f}%)\")\n",
        "\n",
        "    print(\"\\nCONCLUSIONS:\")\n",
        "\n",
        "    if val_improvement > 0.01:  # More than 1% improvement\n",
        "        print(f\"  ✓ Using augmented images IMPROVED validation accuracy by {val_improvement*100:.2f}%\")\n",
        "    elif val_improvement < -0.01:\n",
        "        print(f\"  ✗ Using augmented images DECREASED validation accuracy by {abs(val_improvement)*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"  ≈ Using augmented images had MINIMAL IMPACT on validation accuracy\")\n",
        "\n",
        "    if gap_reduction > 0.02:  # Gap reduced by more than 2%\n",
        "        print(f\"  ✓ Augmented images REDUCED overfitting (gap decreased by {gap_reduction*100:.2f}%)\")\n",
        "    elif gap_reduction < -0.02:\n",
        "        print(f\"  ✗ Augmented images INCREASED overfitting (gap increased by {abs(gap_reduction)*100:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  ≈ Augmented images had MINIMAL IMPACT on overfitting\")\n",
        "\n",
        "    # Training set size comparison\n",
        "    print(f\"\\nDATASET SIZES:\")\n",
        "    print(f\"  Baseline likely used:    ~{len(CLASSES_TO_USE) * MAX_IMAGES_PER_CLASS_TRAIN} images\")\n",
        "    print(f\"  Augmented likely used:   More images (original + aug_ prefixed)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzF7c0Qi4PMg"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Size Comparison\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count images with and without augmentation\n",
        "def count_images_by_type(base_dir, classes):\n",
        "    \"\"\"Count original vs augmented images.\"\"\"\n",
        "    original_count = 0\n",
        "    augmented_count = 0\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "        if os.path.exists(class_dir):\n",
        "            all_images = glob.glob(os.path.join(class_dir, '*.*'))\n",
        "            for img in all_images:\n",
        "                if os.path.basename(img).startswith('aug_'):\n",
        "                    augmented_count += 1\n",
        "                else:\n",
        "                    original_count += 1\n",
        "\n",
        "    return original_count, augmented_count\n",
        "\n",
        "train_orig, train_aug = count_images_by_type(BASE_TRAIN_DIR, CLASSES_TO_USE)\n",
        "val_orig, val_aug = count_images_by_type(BASE_VAL_DIR, CLASSES_TO_USE)\n",
        "test_orig, test_aug = count_images_by_type(BASE_TEST_DIR, CLASSES_TO_USE)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "splits = ['Train', 'Validation', 'Test']\n",
        "original = [train_orig, val_orig, test_orig]\n",
        "augmented = [train_aug, val_aug, test_aug]\n",
        "\n",
        "for i, (split, orig, aug) in enumerate(zip(splits, original, augmented)):\n",
        "    ax[i].bar(['Original', 'Augmented', 'Total'],\n",
        "              [orig, aug, orig + aug],\n",
        "              color=['steelblue', 'coral', 'green'])\n",
        "    ax[i].set_title(f'{split} Set')\n",
        "    ax[i].set_ylabel('Number of Images')\n",
        "    ax[i].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Dataset Composition:\")\n",
        "print(f\"\\nTraining Set:\")\n",
        "print(f\"  Original images:   {train_orig}\")\n",
        "print(f\"  Augmented images:  {train_aug}\")\n",
        "print(f\"  Total:             {train_orig + train_aug}\")\n",
        "print(f\"  Augmentation ratio: {train_aug/train_orig:.2f}x\")\n",
        "\n",
        "print(f\"\\nValidation Set:\")\n",
        "print(f\"  Original images:   {val_orig}\")\n",
        "print(f\"  Augmented images:  {val_aug}\")\n",
        "print(f\"  Total:             {val_orig + val_aug}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Original images:   {test_orig}\")\n",
        "print(f\"  Augmented images:  {test_aug}\")\n",
        "print(f\"  Total:             {test_orig + test_aug}\")\n",
        "\n",
        "# Note: Limited by MAX_IMAGES_PER_CLASS_TRAIN\n",
        "print(f\"\\nNote: Training is limited to {MAX_IMAGES_PER_CLASS_TRAIN} images per class\")\n",
        "print(f\"  With IMG_AUGMENT=False: Uses only original images\")\n",
        "print(f\"  With IMG_AUGMENT=True:  Uses only augmented images\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}