{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garlicxd/Fruit-Classification/blob/automation/Fruit_Classification_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE5TzdybiHaZ"
      },
      "source": [
        "## Todo\n",
        "- [ ] patience for training - stopping after no improvement for several epochs\n",
        "- [ ] create graphs - store information while training\n",
        "- [ ] compare different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "805e28f2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Function Declarations\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration Class\n",
        "# ==========================================\n",
        "class Config:\n",
        "    def __init__(self, defaults, params=None, base_experiment_dir=None):\n",
        "        \"\"\"\n",
        "        Initializes Config.\n",
        "        Args:\n",
        "            defaults (dict): Dictionary where Key -> (Value, Suffix_String).\n",
        "                             Suffix_String is used for directory naming.\n",
        "                             If Suffix_String is \"\", it is NOT added to naming_map.\n",
        "            params (dict): Dictionary where Key -> Value (Overrides defaults).\n",
        "            base_experiment_dir (str): Parent directory for output.\n",
        "        \"\"\"\n",
        "        self.config_values = {}\n",
        "        self.naming_map = {}\n",
        "\n",
        "        # 1. Parse Defaults (Separating Values and Naming Suffixes)\n",
        "        for key, item in defaults.items():\n",
        "            # Assume item is a tuple of length 2: (Value, \"Suffix\")\n",
        "            val, suffix = item\n",
        "            self.config_values[key] = val\n",
        "\n",
        "            # Only add to naming_map if suffix is not empty\n",
        "            if suffix:\n",
        "                self.naming_map[key] = suffix\n",
        "\n",
        "        # 2. Update with specific Params (Value overrides)\n",
        "        if params:\n",
        "            self.config_values.update(params)\n",
        "\n",
        "        # 3. Set attributes dynamically\n",
        "        for key, val in self.config_values.items():\n",
        "            setattr(self, key, val)\n",
        "\n",
        "        # 4. Handle Derived Attributes\n",
        "        if hasattr(self, 'CLASSES_TO_USE'):\n",
        "            self.NUM_CLASSES = len(self.CLASSES_TO_USE)\n",
        "\n",
        "        if hasattr(self, 'DEVICE') and isinstance(self.DEVICE, str):\n",
        "            self.DEVICE = torch.device(self.DEVICE)\n",
        "\n",
        "        # 5. Automate DIR_NAME creation\n",
        "        name_parts = []\n",
        "        # Iterate only through keys present in naming_map (sorted for consistency)\n",
        "        for key in sorted(self.naming_map.keys()):\n",
        "            suffix = self.naming_map[key]\n",
        "            val = self.config_values[key]\n",
        "            name_parts.append(f\"{val}{suffix}\")\n",
        "\n",
        "        self.DIR_NAME = \"_\".join(name_parts) if name_parts else \"default_run\"\n",
        "\n",
        "        # 6. Setup Output Directory\n",
        "        self.base_experiment_dir = base_experiment_dir if base_experiment_dir else \".\"\n",
        "        self.OUTPUT_DIR = os.path.join(self.base_experiment_dir, self.DIR_NAME)\n",
        "\n",
        "        # Derived Paths for Data\n",
        "        self.BASE_TRAIN_DIR = os.path.join(self.DATASET_FOLDER_PATH, 'Train_Set_Folder')\n",
        "        self.BASE_VAL_DIR = os.path.join(self.DATASET_FOLDER_PATH, 'Validation_Set_Folder')\n",
        "        self.BASE_TEST_DIR = os.path.join(self.DATASET_FOLDER_PATH, 'Test_Set_Folder')\n",
        "\n",
        "        self.FILTERED_TRAIN_DIR = os.path.join(self.BASE_FILTERED_DIR, 'train')\n",
        "        self.FILTERED_VAL_DIR = os.path.join(self.BASE_FILTERED_DIR, 'val')\n",
        "        self.FILTERED_TEST_DIR = os.path.join(self.BASE_FILTERED_DIR, 'test')\n",
        "\n",
        "    def print_summary(self):\n",
        "        print(f\"\\n--- Configuration Summary: {self.DIR_NAME} ---\")\n",
        "        print(f\"Output Directory: {self.OUTPUT_DIR}\")\n",
        "        print(f\"Device: {self.DEVICE}\")\n",
        "        # Print only keys that determine the experiment name\n",
        "        for k in self.naming_map:\n",
        "            print(f\"{k}: {getattr(self, k)}\")\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "    def save_config(self):\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "        # Filter for uppercase keys (standard config params)\n",
        "        config_dict = {k: str(v) for k, v in self.__dict__.items() if k.isupper()}\n",
        "        path = os.path.join(self.OUTPUT_DIR, 'config.json')\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=4)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preparation\n",
        "# ==========================================\n",
        "def setup_dataset(config):\n",
        "    if not os.path.exists(config.DATASET_FOLDER_PATH):\n",
        "        print(\"Dataset not found. Downloading...\")\n",
        "        try:\n",
        "            subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"yudhaislamisulistya/plants-type-datasets\"], check=True)\n",
        "            import zipfile\n",
        "            with zipfile.ZipFile(\"plants-type-datasets.zip\", 'r') as zip_ref:\n",
        "                zip_ref.extractall(\".\")\n",
        "            print(\"Dataset downloaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading dataset: {e}\")\n",
        "\n",
        "def create_filtered_subsets(config):\n",
        "    if os.path.exists(config.BASE_FILTERED_DIR):\n",
        "        shutil.rmtree(config.BASE_FILTERED_DIR)\n",
        "\n",
        "    def process_split(src_base, dst_base, is_train=False):\n",
        "        if not os.path.exists(src_base): return\n",
        "        os.makedirs(dst_base, exist_ok=True)\n",
        "\n",
        "        for class_name in config.CLASSES_TO_USE:\n",
        "            src_dir = os.path.join(src_base, class_name)\n",
        "            dst_dir = os.path.join(dst_base, class_name)\n",
        "            os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "            if not os.path.exists(src_dir): continue\n",
        "\n",
        "            all_images = glob.glob(os.path.join(src_dir, '*.*'))\n",
        "\n",
        "            if config.IMG_AUGMENT:\n",
        "                imgs = all_images\n",
        "            else:\n",
        "                imgs = [img for img in all_images if not os.path.basename(img).startswith('aug_')]\n",
        "\n",
        "            if is_train:\n",
        "                imgs = imgs[:config.MAX_IMAGES_PER_CLASS_TRAIN]\n",
        "\n",
        "            for img_path in imgs:\n",
        "                shutil.copy(img_path, dst_dir)\n",
        "\n",
        "    print(\"Creating filtered dataset...\")\n",
        "    process_split(config.BASE_TRAIN_DIR, config.FILTERED_TRAIN_DIR, is_train=True)\n",
        "    process_split(config.BASE_VAL_DIR, config.FILTERED_VAL_DIR)\n",
        "    process_split(config.BASE_TEST_DIR, config.FILTERED_TEST_DIR)\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    imgnet_mean, imgnet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "    transforms_common = transforms.Compose([\n",
        "        transforms.Resize(config.IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imgnet_mean, imgnet_std)\n",
        "    ])\n",
        "\n",
        "    data_transforms = {x: transforms_common for x in ['train', 'val', 'test']}\n",
        "\n",
        "    image_datasets = {\n",
        "        x: datasets.ImageFolder(getattr(config, f'FILTERED_{x.upper()}_DIR'), data_transforms[x])\n",
        "        for x in ['train', 'val', 'test']\n",
        "    }\n",
        "\n",
        "    use_pin = (config.DEVICE.type == 'cuda')\n",
        "\n",
        "    dataloaders = {\n",
        "        x: DataLoader(image_datasets[x], batch_size=config.BATCH_SIZE,\n",
        "                      shuffle=(x=='train' and config.SHUFFLE_TRAINING),\n",
        "                      pin_memory=use_pin, num_workers=2)\n",
        "        for x in ['train', 'val', 'test']\n",
        "    }\n",
        "\n",
        "    sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "    return dataloaders, sizes\n",
        "\n",
        "# ==========================================\n",
        "# 3. Model & Training\n",
        "# ==========================================\n",
        "def build_model(config):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, config.NUM_CLASSES)\n",
        "    model = model.to(config.DEVICE)\n",
        "    return model\n",
        "\n",
        "def train_model(config, model, dataloaders, dataset_sizes):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.fc.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    history = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
        "    best_acc = 0.0\n",
        "    best_weights = model.state_dict()\n",
        "\n",
        "    print(f\"Starting training for {config.EPOCHS} epochs...\")\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        model.train()\n",
        "        running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "        for inputs, labels in dataloaders['train']:\n",
        "            inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(torch.max(outputs, 1)[1] == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes['train']\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
        "\n",
        "        model.eval()\n",
        "        val_loss_running, val_corrects = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloaders['val']:\n",
        "                inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss_running += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(torch.max(outputs, 1)[1] == labels.data)\n",
        "\n",
        "        val_loss = val_loss_running / dataset_sizes['val']\n",
        "        val_acc = val_corrects.double() / dataset_sizes['val']\n",
        "\n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['accuracy'].append(epoch_acc.item())\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_acc.item())\n",
        "\n",
        "        print(f\"Ep {epoch+1}: Train Loss {epoch_loss:.4f} Acc {epoch_acc:.4f} | Val Loss {val_loss:.4f} Acc {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_weights = model.state_dict()\n",
        "\n",
        "    model.load_state_dict(best_weights)\n",
        "    return model, history\n",
        "\n",
        "def evaluate_and_save(config, model, dataloaders, dataset_sizes, history):\n",
        "    model.eval()\n",
        "    test_corrects = 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['test']:\n",
        "            inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    test_acc = test_corrects.double() / dataset_sizes['test']\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(config.OUTPUT_DIR, 'model.pth'))\n",
        "\n",
        "    with open(os.path.join(config.OUTPUT_DIR, 'history.json'), 'w') as f:\n",
        "        json.dump(history, f)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['accuracy'], label='Train')\n",
        "    plt.plot(history['val_accuracy'], label='Val')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['loss'], label='Train')\n",
        "    plt.plot(history['val_loss'], label='Val')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(config.OUTPUT_DIR, 'training_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(config.NUM_CLASSES)\n",
        "    plt.xticks(tick_marks, config.CLASSES_TO_USE, rotation=90)\n",
        "    plt.yticks(tick_marks, config.CLASSES_TO_USE)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# 4. Orchestrators\n",
        "# ==========================================\n",
        "def get_next_experiment_dir(base_path=\".\"):\n",
        "    existing_dirs = glob.glob(os.path.join(base_path, \"experiment_*\"))\n",
        "    max_num = 0\n",
        "    for d in existing_dirs:\n",
        "        try:\n",
        "            num = int(os.path.basename(d).split('_')[1])\n",
        "            if num > max_num: max_num = num\n",
        "        except (IndexError, ValueError):\n",
        "            pass\n",
        "    return os.path.join(base_path, f\"experiment_{max_num + 1}\")\n",
        "\n",
        "def run_with_config(defaults, params=None, base_experiment_dir=None):\n",
        "    cfg = Config(defaults=defaults, params=params, base_experiment_dir=base_experiment_dir)\n",
        "    cfg.print_summary()\n",
        "    cfg.save_config()\n",
        "\n",
        "    setup_dataset(cfg)\n",
        "    create_filtered_subsets(cfg)\n",
        "    dataloaders, sizes = get_dataloaders(cfg)\n",
        "\n",
        "    model = build_model(cfg)\n",
        "    model, history = train_model(cfg, model, dataloaders, sizes)\n",
        "\n",
        "    evaluate_and_save(cfg, model, dataloaders, sizes, history)\n",
        "\n",
        "def run_all(defaults, grid, base_experiment_dir=None):\n",
        "    keys, values = zip(*grid.items())\n",
        "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    print(f\"Total experiments to run: {len(combinations)}\")\n",
        "\n",
        "    if base_experiment_dir is None:\n",
        "        base_experiment_dir = get_next_experiment_dir()\n",
        "\n",
        "    os.makedirs(base_experiment_dir, exist_ok=True)\n",
        "    print(f\"All runs will be saved to: {base_experiment_dir}\")\n",
        "\n",
        "    for i, params in enumerate(combinations):\n",
        "        print(f\"\\n>>> Running Experiment {i+1}/{len(combinations)}\")\n",
        "        run_with_config(defaults=defaults, params=params, base_experiment_dir=base_experiment_dir)\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default Config - set folder naming\n",
        "# Format: \"KEY\": (Value, \"suffix\")\n",
        "# If suffix is \"\", it is excluded from the naming map.\n",
        "default_config_tuples = {\n",
        "    \"DEVICE\": (\"cuda\" if torch.cuda.is_available() else \"cpu\", \"\"),\n",
        "    \"DATASET_FOLDER_PATH\": (\"./split_ttv_dataset_type_of_plants\", \"\"),\n",
        "    \"BASE_FILTERED_DIR\": (\"./filtered_data\", \"\"),\n",
        "    \"IMG_SIZE\": ((224, 224), \"\"),\n",
        "    \"BATCH_SIZE\": (32, \"bs\"),          # Included in dir name: \"32bs\"\n",
        "    \"LEARNING_RATE\": (0.01, \"lr\"),     # Included in dir name: \"0.01lr\"\n",
        "    \"EPOCHS\": (5, \"\"),\n",
        "    \"MAX_IMAGES_PER_CLASS_TRAIN\": (1000, \"\"),\n",
        "    \"SHUFFLE_TRAINING\": (True, \"\"),\n",
        "    \"IMG_AUGMENT\": (False, \"aug\"),     # Included in dir name: \"Falseaug\"\n",
        "    \"CLASSES_TO_USE\": ([\n",
        "        \"aloevera\", \"banana\", \"bilimbi\", \"cantaloupe\", \"cassava\", \"coconut\",\n",
        "        \"corn\", \"cucumber\", \"curcuma\", \"eggplant\", \"galangal\", \"ginger\",\n",
        "        \"guava\", \"kale\", \"longbeans\", \"mango\", \"melon\", \"orange\", \"paddy\",\n",
        "        \"papaya\", \"peper chili\", \"pineapple\", \"pomelo\", \"shallot\", \"soybeans\",\n",
        "        \"spinach\", \"sweet potatoes\", \"tobacco\", \"waterapple\", \"watermelon\"\n",
        "    ], \"\")\n",
        "}"
      ],
      "metadata": {
        "id": "U8fTM3mB0rxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solo Run Default Config\n",
        "run_with_config(\n",
        "    defaults=default_config_tuples,\n",
        "    params=None,\n",
        "    base_experiment_dir=\"solo_run\"\n",
        ")"
      ],
      "metadata": {
        "id": "DKgMky3t0HAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Automated Run - add the value options to the grid\n",
        "grid = {\n",
        "    \"LEARNING_RATE\": [0.01, 0.005],\n",
        "    \"BATCH_SIZE\": [16, 32],\n",
        "    \"IMG_AUGMENT\": [True, False]\n",
        "}\n",
        "\n",
        "# 3. Run Automation\n",
        "run_all(defaults=default_config_tuples, grid=grid)"
      ],
      "metadata": {
        "id": "MPCt4XUB0V79"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}